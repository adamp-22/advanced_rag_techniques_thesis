{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432c9df1",
   "metadata": {},
   "source": [
    "# RAPTOR (Recursive Abstraction and Partitioning for Tree-based Organized Retrieval)\n",
    "\n",
    "\n",
    "**RAPTOR (Recursive Abstraction and Partitioning for Tree-based Organized Retrieval)** is\n",
    "another hierarchical document summarisation technique which could enhance the performance\n",
    "\n",
    "of a RAG system. This method implements a tree structure which captures both high and low-\n",
    "level details in the knowledge base. In order to achieve this, this approach groups text into\n",
    "\n",
    "clusters, summarises each cluster and iteratively repeats this process while building a tree from\n",
    "the ground up. This way, RAPTOR allows the system to incorporate knowledge on multiple\n",
    "levels, traversing from high-level summaries down to detailed passages, enhancing the quality\n",
    "of retrieved context. Furthermore, besides this invaluable structure, the retrieval is also\n",
    "alleviated by the introduced clustering method, which organises contextually similar\n",
    "knowledge in the same group. This is expected to improve coherence by margins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dfc6083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variables Set:\n",
      "LANGSMITH_TRACING: true\n",
      "LANGSMITH_ENDPOINT: https://api.smith.langchain.com\n",
      "LANGSMITH_PROJECT: RAPTOR\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"RAPTOR\"\n",
    "\n",
    "# Verify that the environment variables are set\n",
    "print(\"Environment Variables Set:\")\n",
    "print(f'LANGSMITH_TRACING: {os.getenv(\"LANGSMITH_TRACING\")}')\n",
    "print(f'LANGSMITH_ENDPOINT: {os.getenv(\"LANGSMITH_ENDPOINT\")}')\n",
    "print(f'LANGSMITH_PROJECT: {os.getenv(\"LANGSMITH_PROJECT\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81d47a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langsmith import traceable\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.schema import AIMessage\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7293db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd5620",
   "metadata": {},
   "source": [
    "## LLM and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f91254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=4000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f953df",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb873bc9",
   "metadata": {},
   "source": [
    "## Building the RAPTOR structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbef398",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def extract_text(item):\n",
    "    \"\"\"Extract text content from either a string or an AIMessage object.\"\"\"\n",
    "    if isinstance(item, AIMessage):\n",
    "        return item.content\n",
    "    return item\n",
    "\n",
    "@traceable\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Embed texts using OpenAIEmbeddings.\"\"\"\n",
    "    logging.info(f\"Embedding {len(texts)} texts\")\n",
    "    return embeddings.embed_documents([extract_text(text) for text in texts])\n",
    "\n",
    "@traceable\n",
    "def perform_clustering(embeddings: np.ndarray, n_clusters: int = 10) -> np.ndarray:\n",
    "    \"\"\"Perform clustering on embeddings using Gaussian Mixture Model.\"\"\"\n",
    "    logging.info(f\"Performing clustering with {n_clusters} clusters\")\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "    return gm.fit_predict(embeddings)\n",
    "\n",
    "@traceable\n",
    "def summarize_texts(texts: List[str]) -> str:\n",
    "    \"\"\"Summarize a list of texts using OpenAI.\"\"\"\n",
    "    logging.info(f\"Summarizing {len(texts)} texts\")\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Summarize the following text concisely:\\n\\n{text}\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    input_data = {\"text\": texts}\n",
    "    return chain.invoke(input_data)\n",
    "\n",
    "@traceable\n",
    "def visualize_clusters(embeddings: np.ndarray, labels: np.ndarray, level: int):\n",
    "    \"\"\"Visualize clusters using PCA.\"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(f'Cluster Visualization - Level {level}')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86580657",
   "metadata": {},
   "source": [
    "## The Core Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def build_raptor_tree(texts: List[str], max_levels: int = 3) -> Dict[int, pd.DataFrame]:\n",
    "    \"\"\"Build the RAPTOR tree structure with level metadata and parent-child relationships.\"\"\"\n",
    "    results = {}\n",
    "    current_texts = [extract_text(text) for text in texts]\n",
    "    current_metadata = [{\"level\": 0, \"origin\": \"original\", \"parent_id\": None} for _ in texts]\n",
    "    \n",
    "    for level in range(1, max_levels + 1):\n",
    "        logging.info(f\"Processing level {level}\")\n",
    "        \n",
    "        embeddings = embed_texts(current_texts)\n",
    "        n_clusters = min(10, len(current_texts) // 2)\n",
    "        cluster_labels = perform_clustering(np.array(embeddings), n_clusters)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'text': current_texts,\n",
    "            'embedding': embeddings,\n",
    "            'cluster': cluster_labels,\n",
    "            'metadata': current_metadata\n",
    "        })\n",
    "        \n",
    "        results[level-1] = df\n",
    "        \n",
    "        summaries = []\n",
    "        new_metadata = []\n",
    "        for cluster in df['cluster'].unique():\n",
    "            cluster_docs = df[df['cluster'] == cluster]\n",
    "            cluster_texts = cluster_docs['text'].tolist()\n",
    "            cluster_metadata = cluster_docs['metadata'].tolist()\n",
    "            summary = summarize_texts(cluster_texts)\n",
    "            summaries.append(summary)\n",
    "            new_metadata.append({\n",
    "                \"level\": level,\n",
    "                \"origin\": f\"summary_of_cluster_{cluster}_level_{level-1}\",\n",
    "                \"child_ids\": [meta.get('id') for meta in cluster_metadata],\n",
    "                \"id\": f\"summary_{level}_{cluster}\"\n",
    "            })\n",
    "        \n",
    "        current_texts = summaries\n",
    "        current_metadata = new_metadata\n",
    "        \n",
    "        if len(current_texts) <= 1:\n",
    "            results[level] = pd.DataFrame({\n",
    "                'text': current_texts,\n",
    "                'embedding': embed_texts(current_texts),\n",
    "                'cluster': [0],\n",
    "                'metadata': current_metadata\n",
    "            })\n",
    "            logging.info(f\"Stopping at level {level} as we have only one summary\")\n",
    "            break\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea85072",
   "metadata": {},
   "source": [
    "## Vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f3712",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def create_vector_store(docs, save_path=\"nvidia_RAPTOR\"):\n",
    "    \"\"\"Create and save a FAISS vector store from given documents.\"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    vectorstore.save_local(save_path)\n",
    "    logging.info(f\"FAISS vectorstore saved to {save_path}\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def build_vectorstore(tree_results: Dict[int, pd.DataFrame], save_path=\"nvidia_RAPTOR\") -> FAISS:\n",
    "    \"\"\"Build a FAISS vectorstore from all texts in the RAPTOR tree and save it.\"\"\"\n",
    "    all_texts = []\n",
    "    all_embeddings = []\n",
    "    all_metadatas = []\n",
    "\n",
    "    for level, df in tree_results.items():\n",
    "        all_texts.extend([str(text) for text in df['text'].tolist()])\n",
    "        all_embeddings.extend([embedding.tolist() if isinstance(embedding, np.ndarray) else embedding for embedding in df['embedding'].tolist()])\n",
    "        all_metadatas.extend(df['metadata'].tolist())\n",
    "\n",
    "    logging.info(f\"Building vectorstore with {len(all_texts)} texts\")\n",
    "\n",
    "    # Create Document objects manually to ensure correct types\n",
    "    documents = [Document(page_content=str(text), metadata=metadata)\n",
    "                 for text, metadata in zip(all_texts, all_metadatas)]\n",
    "\n",
    "    return create_vector_store(documents, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea9a49",
   "metadata": {},
   "source": [
    "## Define tree traversal retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def tree_traversal_retrieval(query: str, vectorstore: FAISS, k: int = 3) -> List[Document]:\n",
    "    \"\"\"Perform tree traversal retrieval.\"\"\"\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    \n",
    "    def retrieve_level(level: int, parent_ids: List[str] = None) -> List[Document]:\n",
    "        if parent_ids:\n",
    "            docs = vectorstore.similarity_search_by_vector_with_relevance_scores(\n",
    "                query_embedding,\n",
    "                k=k,\n",
    "                filter=lambda meta: meta['level'] == level and meta['id'] in parent_ids\n",
    "            )\n",
    "        else:\n",
    "            docs = vectorstore.similarity_search_by_vector_with_relevance_scores(\n",
    "                query_embedding,\n",
    "                k=k,\n",
    "                filter=lambda meta: meta['level'] == level\n",
    "            )\n",
    "        \n",
    "        if not docs or level == 0:\n",
    "            return docs\n",
    "        \n",
    "        child_ids = [doc.metadata.get('child_ids', []) for doc, _ in docs]\n",
    "        child_ids = [item for sublist in child_ids for item in sublist]  # Flatten the list\n",
    "        \n",
    "        child_docs = retrieve_level(level - 1, child_ids)\n",
    "        return docs + child_docs\n",
    "    \n",
    "    max_level = max(doc.metadata['level'] for doc in vectorstore.docstore.values())\n",
    "    return retrieve_level(max_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de5044",
   "metadata": {},
   "source": [
    "## Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad61280",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def create_retriever(vectorstore: FAISS) -> ContextualCompressionRetriever:\n",
    "    \"\"\"Create a retriever with contextual compression.\"\"\"\n",
    "    base_retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Given the following context and question, extract only the relevant information for answering the question:\\n\\n\"\n",
    "        \"Context: {context}\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Relevant Information:\"\n",
    "    )\n",
    "    \n",
    "    extractor = LLMChainExtractor.from_llm(llm, prompt=prompt)\n",
    "    \n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor=extractor,\n",
    "        base_retriever=base_retriever\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04271cda",
   "metadata": {},
   "source": [
    "## Define hierarchical retraieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def hierarchical_retrieval(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> List[Document]:\n",
    "    \"\"\"Perform hierarchical retrieval starting from the highest level, handling potential None values.\"\"\"\n",
    "    all_retrieved_docs = []\n",
    "    \n",
    "    for level in range(max_level, -1, -1):\n",
    "        # Retrieve documents from the current level\n",
    "        level_docs = retriever.get_relevant_documents(\n",
    "            query,\n",
    "            filter=lambda meta: meta['level'] == level\n",
    "        )\n",
    "        all_retrieved_docs.extend(level_docs)\n",
    "        \n",
    "        # If we've found documents, retrieve their children from the next level down\n",
    "        if level_docs and level > 0:\n",
    "            child_ids = [doc.metadata.get('child_ids', []) for doc in level_docs]\n",
    "            child_ids = [item for sublist in child_ids for item in sublist if item is not None]  # Flatten and filter None\n",
    "            \n",
    "            if child_ids:  # Only modify query if there are valid child IDs\n",
    "                child_query = f\" AND id:({' OR '.join(str(id) for id in child_ids)})\"\n",
    "                query += child_query\n",
    "    \n",
    "    return all_retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287fd8b9",
   "metadata": {},
   "source": [
    "## Load the Vector Store Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dee17fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_vectorstore(save_path=\"nvidia_large_baseline\") -> FAISS:\n",
    "    \"\"\"Load an existing FAISS vector store from disk.\"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    vectorstore = FAISS.load_local(\n",
    "        save_path,\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b51914",
   "metadata": {},
   "source": [
    "## Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e8cabdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(vectorstore: FAISS) -> ContextualCompressionRetriever:\n",
    "    \"\"\"Create a retriever with contextual compression.\"\"\"\n",
    "    logging.info(\"Creating contextual compression retriever\")\n",
    "    base_retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Given the following context and question, extract only the relevant information for answering the question:\\n\\n\"\n",
    "        \"Context: {context}\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Relevant Information:\"\n",
    "    )\n",
    "    \n",
    "    extractor = LLMChainExtractor.from_llm(llm, prompt=prompt)\n",
    "    \n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor=extractor,\n",
    "        base_retriever=base_retriever\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7270577",
   "metadata": {},
   "source": [
    "## RAPTOR Query Process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6915f043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import logging\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "\n",
    "@traceable\n",
    "def raptor_query(\n",
    "    query: str, \n",
    "    retriever: BaseRetriever, \n",
    "    max_level: int, \n",
    "    llm: Any\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a query using an advanced RAG approach with hierarchical retrieval.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The input query to process\n",
    "        retriever (BaseRetriever): The retriever to use for document retrieval\n",
    "        max_level (int): Maximum hierarchical retrieval depth\n",
    "        llm: The language model to use for generation\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing query results and metadata\n",
    "    \"\"\"\n",
    "    logging.info(f\"Processing query: {query}\")\n",
    "    \n",
    "    # Retrieve documents\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    \n",
    "    # Format context\n",
    "    def format_docs(docs):\n",
    "        return [doc.page_content for doc in docs] \n",
    "    \n",
    "    context = format_docs(retrieved_docs)\n",
    "    \n",
    "    # Define the RAG prompt\n",
    "    rag_prompt = ChatPromptTemplate.from_template(\n",
    "        \"You are an expert assistant. Answer the question based on the following context:\\n\\n\"\n",
    "        \"Context:\\n{context}\\n\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Provide a detailed and accurate answer.\"\n",
    "    )\n",
    "    \n",
    "    # Create the RAG chain\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Execute the chain\n",
    "    try:\n",
    "        # Generate answer\n",
    "        answer = rag_chain.invoke(query)\n",
    "        \n",
    "        # Prepare result dictionary\n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"retrieved_documents\": [\n",
    "                {\n",
    "                    \"index\": i + 1,\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata,\n",
    "                    \"level\": doc.metadata.get('level', 'Unknown'),\n",
    "                    \"similarity_score\": doc.metadata.get('score', 'N/A')\n",
    "                } for i, doc in enumerate(retrieved_docs)\n",
    "            ],\n",
    "            \"num_docs_retrieved\": len(retrieved_docs),\n",
    "            \"contexts\": context,  # Explicitly include the full context\n",
    "            \"answer\": answer,\n",
    "            \"model_used\": getattr(llm, 'model_name', 'Unknown'),\n",
    "        }\n",
    "        \n",
    "        logging.info(\"Query processing completed successfully\")\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing query: {str(e)}\")\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c5cda7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_query_details(result: Dict[str, Any]):\n",
    "    \"\"\"Print detailed information about the query process, including tree level metadata.\"\"\"\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"\\nNumber of documents retrieved: {result['num_docs_retrieved']}\")\n",
    "    print(f\"\\nRetrieved Documents:\")\n",
    "    for doc in result['retrieved_documents']:\n",
    "        print(f\"  Document {doc['index']}:\")\n",
    "        print(f\"    Content: {doc['content'][:100]}...\")  # Show first 100 characters\n",
    "        print(f\"    Similarity Score: {doc['similarity_score']}\")\n",
    "        print(f\"    Tree Level: {doc['metadata'].get('level', 'Unknown')}\")\n",
    "        print(f\"    Origin: {doc['metadata'].get('origin', 'Unknown')}\")\n",
    "        if 'child_docs' in doc['metadata']:\n",
    "            print(f\"    Number of Child Documents: {len(doc['metadata']['child_docs'])}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"\\nContext used for answer generation:\")\n",
    "    print(result['contexts'])\n",
    "    \n",
    "    print(f\"\\nGenerated Answer:\")\n",
    "    print(result['answer'])\n",
    "    \n",
    "    print(f\"\\nModel Used: {result['model_used']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5b466d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 15:13:19,711 - INFO - Loading existing FAISS vectorstore from nvidia_RAPTOR\n",
      "2025-03-04 15:13:19,821 - INFO - Loading faiss with AVX2 support.\n",
      "2025-03-04 15:13:19,869 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-03-04 15:13:19,883 - INFO - Successfully loaded FAISS vectorstore\n"
     ]
    }
   ],
   "source": [
    "vectorstore = load_existing_vectorstore(\"nvidia_RAPTOR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73311fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 15:13:21,219 - INFO - Creating contextual compression retriever\n"
     ]
    }
   ],
   "source": [
    "# Create retriever\n",
    "retriever = create_retriever(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "719c2e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 15:13:22,802 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:13:22,804 - INFO - Processing query: Who is the CEO of NVIDIA?\n",
      "2025-03-04 15:13:24,020 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:13:25,043 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:13:25,769 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:13:26,682 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:13:27,517 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:14:06,106 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:14:07,343 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:14:08,109 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:14:09,075 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:14:09,995 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:14:11,539 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:14:11,561 - INFO - Query processing completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who is the CEO of NVIDIA?\n",
      "\n",
      "Number of documents retrieved: 4\n",
      "\n",
      "Retrieved Documents:\n",
      "  Document 1:\n",
      "    Content: Jen-Hsun Huang is the President and Chief Executive Officer of NVIDIA....\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 2:\n",
      "    Content: Jen-Hsun Huang is the President and Chief Executive Officer of NVIDIA Corporation....\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 3:\n",
      "    Content: Jen-Hsun Huang is the CEO of NVIDIA....\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 4:\n",
      "    Content: Jen-Hsun Huang is the CEO of NVIDIA....\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "\n",
      "Context used for answer generation:\n",
      "['Jen-Hsun Huang is the President and Chief Executive Officer of NVIDIA.', 'Jen-Hsun Huang is the President and Chief Executive Officer of NVIDIA Corporation.', 'Jen-Hsun Huang is the CEO of NVIDIA.', 'Jen-Hsun Huang is the CEO of NVIDIA.']\n",
      "\n",
      "Generated Answer:\n",
      "The CEO of NVIDIA is Jen-Hsun Huang. He holds the position of President and Chief Executive Officer of NVIDIA Corporation, which is commonly referred to simply as NVIDIA. His leadership role is emphasized in multiple statements, confirming his status as the head of the company.\n",
      "\n",
      "Model Used: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "max_level = 3  # Adjust based on your tree depth\n",
    "query = \"Who is the CEO of NVIDIA?\"\n",
    "result = raptor_query(query, retriever, max_level, llm)\n",
    "print_query_details(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f032be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jen-Hsun Huang is the President and Chief Executive Officer of NVIDIA.',\n",
       " 'Jen-Hsun Huang is the President and Chief Executive Officer of NVIDIA Corporation.',\n",
       " 'Jen-Hsun Huang is the CEO of NVIDIA.',\n",
       " 'Jen-Hsun Huang is the CEO of NVIDIA.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['contexts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5cbc909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The CEO of NVIDIA is Jen-Hsun Huang. He holds the position of President and Chief Executive Officer of NVIDIA Corporation, which is commonly referred to simply as NVIDIA. His leadership role is emphasized in multiple statements, confirming his status as the head of the company.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c8b66",
   "metadata": {},
   "source": [
    "## RAG Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b38ac157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"What was NVIDIA's revenue in 2024 and 2023? Which year saw higher revenues and by how much?\"}\n",
      "{'answer': \"Revenue in 2024 was $60,922 million ($60.9 billion) , revenue in 2023 was $26,974 million ($27.0 billion). NVIDIA's revenue increased by $33,948 million (or $33.95 billion depending on the rounding) in 2024 compared to 2023.\"}\n",
      "{'question': 'Between January 29, 2023, and January 28, 2024, NVIDIA’s total assets increased by approximately $24.5 billion. Based on the balance sheet, which two line items contributed the most to this increase, and what does this suggest about the company’s capital deployment focus?'}\n",
      "{'answer': 'Between January 29, 2023, and January 28, 2024, NVIDIA’s total assets increased by approximately $24.5 billion, rising from $41.2 billion to $65.7 billion. Based on the balance sheet, the two line items that contributed the most to this increase were marketable securities, which increased from $9.9 billion to $18.7 billion, and cash and cash equivalents, which increased from $3.4 billion to $7.3 billion. Together, these two line items account for approximately $13.7 billion of the total asset increase. This suggests that NVIDIA’s capital deployment focus emphasized preserving liquidity and flexibility, likely to ensure the company maintains ample financial resources for future strategic opportunities, such as potential acquisitions, research and development investments, or shareholder returns. The strong growth in these liquid assets highlights a preference for maintaining a robust financial position, possibly in response to the dynamic technology landscape and potential geopolitical or economic uncertainties.\\n'}\n",
      "{'question': \"In fiscal year 2024, NVIDIA generated a substantial increase in operating cash flow compared to fiscal year 2023. However, despite this cash inflow, the company's cash balance at year-end only increased modestly. Analyze the main drivers behind this modest cash increase and discuss what this reveals about NVIDIA’s capital allocation priorities and potential financial strategy.\"}\n",
      "{'answer': \"In fiscal year 2024, NVIDIA generated a substantial increase in operating cash flow compared to fiscal year 2023. However, despite this cash inflow, the company's cash balance at year-end only increased modestly. Net cash provided by operating activities increased from $5.6 billion in FY23 to $28.1 billion in FY24 — an increase of over $22 billion. This massive improvement reflects the sharp rise in net income (from $4.4 billion to $29.8 billion), combined with adjustments like $3.5 billion in stock-based compensation and favorable working capital movements (notably accounts payable and accrued liabilities). Despite this influx, NVIDIA’s cash balance only increased from $3.39 billion to $7.28 billion — a $3.89 billion increase. This is modest compared to the $28.1 billion operating cash inflow. $18.2 billion was spent on marketable securities purchases, exceeding maturities and sales (combined inflow of just $9.78 billion). $9.53 billion was spent on stock repurchases, showing NVIDIA’s continued focus on returning capital to shareholders. $2.78 billion was used for tax payments on restricted stock units, another shareholder-related cash outflow. $395 million was paid in dividends. $1.07 billion was spent on capital expenditures. NVIDIA prioritized reinvesting cash into marketable securities, perhaps to preserve flexibility for future strategic opportunities (acquisitions, R&D expansion, etc.). The second major priority was shareholder returns, particularly through aggressive share repurchases and some dividend payments. Capital expenditures were relatively modest compared to the overall cash generation, suggesting that NVIDIA’s core business currently operates with low capital intensity — typical for a fabless semiconductor firm. The combination of aggressive buybacks, tax payments on equity compensation, and heavy marketable securities investment reveals NVIDIA’s financial strategy: balancing short-term shareholder returns with maintaining long-term flexibility for future opportunities. This pattern also highlights that a large portion of the company’s excess cash flow was effectively parked, rather than directly deployed into operating or capital investments\\n\"}\n",
      "{'question': \"How does the conflict in Israel effect NVIDIA's business? How many people are affected? What proportion is that relative to NVIDIA's complete workforce?\"}\n",
      "{'answer': 'The conflict in Israel affects NVIDIA’s business by impacting the health and safety of approximately 3,700 employees in the region, who are primarily involved in research and development, operations, and sales and marketing of NVIDIA’s networking products. While NVIDIA’s global supply chain for networking products has not experienced significant impacts, a substantial number of employees in Israel have been called up for active military duty, which has caused and could continue to cause some disruption to product development and operations. NVIDIA’s operating expenses for fiscal year 2024 include costs related to financial support for impacted employees and charitable activity. Although there was no significant impact on business operations or expenses in fiscal year 2024, NVIDIA acknowledges that a prolonged or escalated conflict could impact future product development, operations, revenue, or create other uncertainties for the business.\\nOut of NVIDIA’s approximately 29,600 employees globally at the end of fiscal year 2024, the 3,700 employees in Israel represent roughly 12.5% of the total workforce.\\n'}\n",
      "{'question': 'Who joined NVIDIA latest out of the executive officers and when?'}\n",
      "{'answer': 'The executive officer who joined NVIDIA most recently is Timothy S. Teter, who joined NVIDIA in 2017 as Senior Vice President, General Counsel and Secretary, and became Executive Vice President, General Counsel and Secretaryin February 2018.'}\n",
      "{'question': 'What types of costs does NVIDIA have, and how have these costs changed over time as a percentage of revenue?'}\n",
      "{'answer': \"NVIDIA's costs can be categorized into the following types: cost of revenue, research and development, sales, general and administrative, and in 2023, there was an acquisition termination cost. To assess how these costs have changed over time as a percentage of revenue, the following calculations apply.\\nCost of revenue as a percentage of revenue in 2024 is calculated as 16,621 divided by 60,922, which equals approximately 27.3%. In 2023, it is 11,618 divided by 26,974, equaling approximately 43.1%. In 2022, it is 9,439 divided by 26,914, equaling approximately 35.1%.\\nResearch and development as a percentage of revenue in 2024 is calculated as 8,675 divided by 60,922, which equals approximately 14.2%. In 2023, it is 7,339 divided by 26,974, equaling approximately 27.2%. In 2022, it is 5,268 divided by 26,914, equaling approximately 19.6%.\\nSales, general and administrative as a percentage of revenue in 2024 is calculated as 2,654 divided by 60,922, which equals approximately 4.4%. In 2023, it is 2,440 divided by 26,974, equaling approximately 9.0%. In 2022, it is 2,166 divided by 26,914, equaling approximately 8.0%.\\nThe acquisition termination cost only occurred in 2023 and as a percentage of revenue is calculated as 1,353 divided by 26,974, equaling approximately 5.0%.\\nOver time, cost of revenue has decreased significantly as a percentage of revenue, from about 35.1% in 2022 to 27.3% in 2024, showing improved gross margin. Research and development costs also declined as a percentage of revenue from 27.2% in 2023 to 14.2% in 2024, though they rose slightly in absolute terms. Sales, general and administrative expenses shrank as a percentage of revenue from 9.0% in 2023 to 4.4% in 2024. In 2023, there was a one-time acquisition termination cost, which was 5.0% of revenue, but this was absent in 2024 and 2022. Overall, NVIDIA’s costs as a proportion of revenue have decreased across all major categories, indicating improved efficiency and profitability in 2024.\\n\"}\n",
      "{'question': 'What is the lifestory of NVIDIA’s CFO, was he/she already at the firm when the Arm Acquisition was terminated?'}\n",
      "{'answer': 'Colette M. Kress joined NVIDIA in 2013 as Executive Vice President and Chief Financial Officer, meaning she was already at the firm when the Arm acquisition was terminated in February 2022. Prior to joining NVIDIA, she served as Senior Vice President and Chief Financial Officer of the Business Technology and Operations Finance organization at Cisco Systems, Inc., where she was responsible for financial strategy, planning, reporting, and business development across all business segments, engineering, and operations. Before Cisco, she spent over a decade at Microsoft, where she held several roles, including Chief Financial Officer of the Server and Tools division starting in 2006, responsible for financial strategy, planning, reporting, and business development for that division. Prior to Microsoft, she spent eight years at Texas Instruments Incorporated in various finance positions. Ms. Kress holds a Bachelor of Science degree in Finance from the University of Arizona and an MBA from Southern Methodist University.'}\n",
      "{'question': 'What was the working capital of NVIDIA in 2024? What does it indicate?'}\n",
      "{'answer': 'NVIDIA’s working capital for the year ending January 28, 2024, was $33,714 million, calculated by subtracting total current liabilities of $10,631 million from total current assets of $44,345 million. This significant positive working capital indicates that NVIDIA has a very strong liquidity position, meaning it can easily cover its short-term obligations. It also reflects financial flexibility, giving NVIDIA the ability to invest in growth opportunities, research, development, and acquisitions without liquidity concerns. Overall, this high working capital highlights NVIDIA’s excellent financial health in terms of short-term solvency and operational strength.\\n'}\n",
      "{'question': 'How does NVIDIA perform across different regions in the global market?'}\n",
      "{'answer': 'Based on the document NVIDIA’s revenue performance across different regions shows significant growth in the United States, with revenue increasing from $8.3 billion in fiscal year 2023 to $27.0 billion in fiscal year 2024, driven primarily by higher demand in the Compute & Networking segment. Taiwan also saw strong revenue growth, increasing from $7.0 billion in fiscal year 2023 to $13.4 billion in fiscal year 2024. Revenue from China, including Hong Kong, increased from $5.8 billion to $10.3 billion over the same period, while revenue from other countries rose from $5.9 billion to $10.2 billion. Although NVIDIA saw strong revenue growth in the U.S., sales to customers outside the United States still accounted for 56% of total revenue in fiscal year 2024, highlighting the company’s significant international presence despite the U.S. becoming a larger revenue source compared to prior years.\\n'}\n",
      "{'question': \"Are there more men or women in NVIDIA's workforce? Are there more technical or non-technical people hired?\"}\n",
      "{'answer': 'There are more men than women in NVIDIA’s workforce, as 79% of the global workforce is male and 20% is female. There are also more technical people than non-technical people hired, as 83% of the workforce is technical.'}\n",
      "{'question': 'How does NVIDIA ensure that its autonomous vehicle platform can meet the safety and complexity requirements of modern AV and EV systems?'}\n",
      "{'answer': 'NVIDIA’s AV platform, branded as DRIVE, is designed and implemented from the ground up based on automotive safety standards. It uses multiple neural networks to handle complex perception, localization, and planning tasks, replacing legacy hand-coded approaches. The platform also includes GPU-based hardware for training and re-simulation to test software updates before deployment.\\n'}\n",
      "{'question': 'How does NVIDIA’s approach to AI training and inferencing across data center-scale solutions demonstrate full-stack innovation, and how does NVIDIA’s ecosystem contribute to reinforcing its leadership position in AI?'}\n",
      "{'answer': 'NVIDIA’s approach to AI training and inferencing demonstrates full-stack innovation by providing a complete, end-to-end accelerated computing platform that spans data center-scale compute and networking solutions across GPUs, CPUs, DPUs, interconnects, systems, and software. This comprehensive coverage allows NVIDIA to optimize performance at every level, from hardware to software, ensuring seamless integration and efficiency for AI workloads. NVIDIA’s ecosystem further reinforces its leadership position in AI through a large and expanding global network of over 4.7 million developers using CUDA and other software tools, along with partnerships with hundreds of universities and thousands of startups through its Inception program, creating a virtuous cycle where broader adoption drives further innovation and leadership.\\n'}\n",
      "{'question': 'List all key competitors identified in NVIDIA’s 10-K filing. What strategies, products, or initiatives are these competitors pursuing to compete with NVIDIA?'}\n",
      "{'answer': 'NVIDIA’s 10-K filing identifies several key competitors across various product categories and industries. These competitors are pursuing a range of strategies, products, and initiatives to compete with NVIDIA.\\nThe key competitors include:\\nAdvanced Micro Devices, Inc. (AMD) – Competing with discrete and integrated GPUs, accelerated computing solutions, and networking products, including DPUs.\\nHuawei Technologies Co. Ltd. – Competing with GPUs, CPUs, and accelerated computing solutions, including AI hardware and software, as well as Arm-based CPUs and networking products.\\nIntel Corporation – Competing with discrete GPUs, custom chips, accelerated computing solutions, and networking products, including DPUs and high-performance interconnect technologies.\\nAlibaba Group – Operating internal teams developing hardware and software solutions that incorporate accelerated or AI computing functionality into their cloud platforms.\\nAlphabet Inc. (Google) – Developing internal hardware and software solutions for accelerated and AI computing, particularly for cloud services.\\nAmazon, Inc. – Developing internal accelerated and AI computing solutions for Amazon Web Services (AWS), as well as Arm-based CPUs for its cloud infrastructure.\\nBaidu, Inc. – Developing internal hardware and software with accelerated and AI computing functionality, particularly for its cloud services and AI platforms.\\nMicrosoft Corporation – Developing internal accelerated and AI computing hardware and software solutions for its Azure cloud platform and incorporating Arm-based CPUs into its solutions.\\nAmbarella, Inc. – Providing SoC products for embedded applications, including autonomous machines and automotive applications.\\nBroadcom Inc. – Competing with SoCs used in servers, embedded devices, and networking products, including high-performance interconnect products such as Ethernet and proprietary technologies.\\nQualcomm Incorporated – Providing SoC products for various embedded applications, including automotive and gaming devices.\\nRenesas Electronics Corporation – Providing SoC products for embedded systems, including automotive applications.\\nSamsung Electronics – Providing SoCs for embedded systems, including gaming devices and autonomous machines.\\nTesla, Inc. – Designing its own internal SoC products for its electric vehicles and autonomous driving systems.\\nArista Networks – Competing in networking products, particularly switches and network adapters.\\nCisco Systems, Inc. – Competing in networking products, including high-performance interconnect technologies.\\nHewlett Packard Enterprise Company – Competing in networking products and data center infrastructure.\\nLumentum Holdings – Competing in optical networking and high-performance interconnect products.\\nMarvell Technology Group – Providing networking products, including high-performance interconnect technologies.\\nAdditionally, system vendors and large cloud service companies are developing internal hardware and software solutions, including networking products and AI computing platforms, to reduce reliance on third-party solutions like NVIDIA’s.\\nThese competitors are pursuing strategies such as developing lower-cost alternatives, offering custom hardware optimized for specific workloads (such as AI training and inference), designing their own internal chips to optimize performance for their platforms, integrating hardware and software tightly into their ecosystems, and expanding their portfolios to include high-performance interconnect solutions and complete systems. Many of these companies also leverage their existing customer bases and ecosystems to drive adoption of their proprietary technologies.\\nIn summary, NVIDIA faces competition from companies developing GPUs, CPUs, DPUs, SoCs, and networking hardware, as well as from large cloud providers with internal chip design teams focused on optimizing AI and accelerated computing performance within their platforms.\\n'}\n",
      "{'question': 'What was the net value of P&E in 2024 and how is it calculated in this specific report?'}\n",
      "{'answer': 'The net value of property and equipment in 2024 was $3,914 million. This value is calculated by taking the total gross value of property and equipment, which was $7,423 million as of January 28, 2024, and subtracting the accumulated depreciation and amortization, which amounted to $3,509 million. This calculation results in the net property and equipment value of $3,914 million, as shown in the report.'}\n",
      "{'question': 'How does NVIDIA’s change in cash and cash equivalents between fiscal year-end 2023 and 2024 compare to the change in total liabilities over the same period? And what does it indicate?'}\n",
      "{'answer': 'Based on NVIDIA’s Consolidated Balance Sheets, NVIDIA’s cash and cash equivalents increased from $3,389 million as of January 29, 2023 to $7,280 million as of January 28, 2024, representing an increase of $3,891 million. Over the same period, total liabilities increased from $19,081 million to $22,750 million, an increase of $3,669 million. This indicates that NVIDIA’s liquidity improved at a slightly faster pace than its liabilities, suggesting a strengthening financial position.'}\n",
      "{'question': 'List all people mentioned in the report. And what are their titles within the NVIDIA?'}\n",
      "{'answer': 'Jen-Hsun Huang – President and Chief Executive Officer, Director (Principal Executive Officer)\\nColette M. Kress – Executive Vice President and Chief Financial Officer (Principal Financial Officer)\\nAjay K. Puri – Executive Vice President, Worldwide Field Operations\\nDebora Shoquist – Executive Vice President, Operations\\nTimothy S. Teter – Executive Vice President and General Counsel\\nDonald Robertson – Vice President and Chief Accounting Officer (Principal Accounting Officer)\\nRobert Burgess – Director\\nTench Coxe – Director\\nJohn O. Dabiri – Director\\nPersis Drell – Director\\nDawn Hudson – Director\\nHarvey C. Jones – Director\\nMelissa B. Lora – Director\\nMichael McCaffery – Director\\nStephen C. Neal – Director\\nMark L. Perry – Director\\nA. Brooke Seawell – Director\\nAarti Shah – Director\\nMark Stevens – Director'}\n",
      "{'question': 'Which products and countries were affected by the recent U.S. export controls, and how did NVIDIA respond?'}\n",
      "{'answer': 'The October 2023 U.S. export controls affected NVIDIA products including A100, A800, H100, H800, L4, L40, L40S, and RTX 4090. These controls applied to exports to China, Hong Kong, and countries in Country Groups D1, D4, and D5. Specific countries mentioned include Saudi Arabia, United Arab Emirates, and Vietnam. In response, NVIDIA transitioned some testing, validation, supply, and distribution operations out of China and Hong Kong. The company also worked to expand its Data Center product portfolio to include alternatives that do not require licenses, and it started shipping alternative products to China in small volumes. Additionally, NVIDIA applied for export licenses where needed, though the process is slow, uncertain, and sometimes includes difficult conditions.'}\n",
      "{'question': 'When did the CEO join NVIDIA?'}\n",
      "{'answer': 'Jen-Hsun Huang co-founded NVIDIA in 1993 and has served as President, Chief Executive Officer, and a member of the Board of Directors since its inception.'}\n",
      "{'question': 'In which large markets does NVIDIA apply its expertise and provide platforms?'}\n",
      "{'answer': \"NVIDIA's platforms address four large markets where our expertise is critical: Data Center, Gaming, Professional Visualization, and Automotive.\"}\n",
      "{'question': 'Who is the CEO of NVIDIA?'}\n",
      "{'answer': 'The CEO of NVIDIA is Jen-Hsun Huang.'}\n"
     ]
    }
   ],
   "source": [
    "# Import the LangSmith client\n",
    "from langsmith import Client\n",
    "\n",
    "# Initialize the LangSmith client\n",
    "ls_client = Client()\n",
    "\n",
    "# Read an existing dataset by name\n",
    "dataset_name = \"RAG_Eval_QA\"\n",
    "dataset = ls_client.read_dataset(dataset_name=dataset_name)\n",
    "\n",
    "# List examples in the dataset\n",
    "examples = ls_client.list_examples(dataset_name=dataset_name)\n",
    "\n",
    "# Iterate through examples\n",
    "for example in examples:\n",
    "    print(example.inputs)  # Print input data\n",
    "    print(example.outputs)  # Print output data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f896cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/66zkglc51d973l0mk64lcrkc0000gn/T/ipykernel_5765/2168577922.py:3: LangChainBetaWarning: Introduced in 0.2.24. API subject to change.\n",
      "  rate_limiter = InMemoryRateLimiter(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=0.1,  # Slow down requests\n",
    "    check_every_n_seconds=0.1,\n",
    "    max_bucket_size=10\n",
    ")\n",
    "\n",
    "# Apply to your model\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0, rate_limiter=rate_limiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38081e16",
   "metadata": {},
   "source": [
    "## RAG Evaluators\n",
    "\n",
    "### Type 1: Reference Answer\n",
    "\n",
    "First, lets consider the case in which we want to compare our RAG chain answer to a reference answer.\n",
    "\n",
    "#### Eval flow\n",
    "\n",
    "We simply use an LLM-as-judge with an easily customized grader prompt: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5143b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG chain\n",
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = raptor_query(example[\"question\"], retriever, max_level, llm)\n",
    "    return {\"answer\": response[\"answer\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0e2d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = raptor_query(example[\"question\"], retriever, max_level, llm)\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad0e70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the LANGSMITH_API_KEY environment variable (create key in settings)\n",
    "from langchain import hub\n",
    "grade_prompt_answer_accuracy = hub.pull(\"answer_vs_reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60f107a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get summary\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    reference = example.outputs[\"answer\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    # llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    \n",
    "    llm = model\n",
    " \n",
    "    # Structured prompt\n",
    "    \n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "739bd306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAPTOR-478de21e' at:\n",
      "https://smith.langchain.com/o/f09c54fc-44b8-4ffe-9055-bc40601f9137/datasets/7fad5e82-7b2d-438e-90e6-1e232f388896/compare?selectedSessions=4dc555d3-eeec-4ea4-9572-a53e194dedef\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67e530659d24a6baed806163fdb4073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:54:24,332 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-03 14:54:24,334 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-03 14:54:24,337 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-03 14:54:24,338 - INFO - Processing query: In which large markets does NVIDIA apply its expertise and provide platforms?\n",
      "2025-03-03 14:54:24,340 - INFO - Processing query: What was NVIDIA's revenue in 2024 and 2023? Which year saw higher revenues and by how much?\n",
      "2025-03-03 14:54:24,341 - INFO - Processing query: Who is the CEO of NVIDIA?\n",
      "2025-03-03 14:54:25,005 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:25,047 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:25,116 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:26,162 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:26,877 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:26,961 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:27,365 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:27,473 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:27,529 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:28,011 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:28,040 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:28,277 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:28,394 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:29,026 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:29,221 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:29,670 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:29,774 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:29,967 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:30,217 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:30,345 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:30,358 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:30,940 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:31,147 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:31,282 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:31,997 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:32,001 - INFO - Query processing completed successfully\n",
      "2025-03-03 14:54:32,159 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:32,159 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:33,020 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:33,330 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:33,942 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:34,119 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:34,534 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:36,194 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:36,201 - INFO - Query processing completed successfully\n",
      "2025-03-03 14:54:37,217 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:37,221 - INFO - Query processing completed successfully\n",
      "2025-03-03 14:54:40,085 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:54:42,349 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "dataset_name = \"RAG_test_NVIDIA\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=\"RAPTOR\",\n",
    "    metadata={\"variant\": \"NVIDIA context, gpt-4o-mini\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988d8e2",
   "metadata": {},
   "source": [
    "### Type 2: Answer Hallucination\n",
    "\n",
    "#### Eval flow\n",
    "\n",
    "We simply use an LLM-as-judge with an easily customized grader prompt: \n",
    "\n",
    "https://smith.langchain.com/hub/langchain-ai/rag-answer-hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a018ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the LANGSMITH_API_KEY environment variable (create key in settings)\n",
    "from langchain import hub\n",
    "grade_prompt_hallucinations = hub.pull(\"hallucination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39e8466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt \n",
    "\n",
    "# grade_prompt_hallucinations = hub.pull(\"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "def answer_hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for generation hallucination\n",
    "    \"\"\"\n",
    "    \n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    contexts = run.outputs[\"contexts\"]\n",
    "        \n",
    "    # RAG answer \n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    # llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    \n",
    "    llm = model\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_hallucinations | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"documents\": contexts,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb954527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAPTOR-18775abf' at:\n",
      "https://smith.langchain.com/o/f09c54fc-44b8-4ffe-9055-bc40601f9137/datasets/7fad5e82-7b2d-438e-90e6-1e232f388896/compare?selectedSessions=b4dac82c-0f01-464c-a8ef-7c2d2ab3d230\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f222d1c765240c797b3d50f77f868a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:56:35,814 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-03 14:56:35,816 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-03 14:56:35,817 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-03 14:56:35,819 - INFO - Processing query: In which large markets does NVIDIA apply its expertise and provide platforms?\n",
      "2025-03-03 14:56:35,820 - INFO - Processing query: What was NVIDIA's revenue in 2024 and 2023? Which year saw higher revenues and by how much?\n",
      "2025-03-03 14:56:35,821 - INFO - Processing query: Who is the CEO of NVIDIA?\n",
      "2025-03-03 14:56:36,199 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:36,428 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:36,546 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:37,203 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:37,267 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:37,825 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:37,826 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:38,250 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:38,738 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:39,281 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:39,296 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:39,875 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:40,065 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:40,723 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:40,723 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:41,243 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:41,800 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:42,865 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:42,870 - INFO - Query processing completed successfully\n",
      "2025-03-03 14:56:42,876 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:43,685 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:44,504 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:45,631 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:46,023 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:46,761 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:46,814 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:47,576 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:48,210 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:48,430 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:49,114 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:49,328 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:50,238 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:51,160 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:52,798 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:52,803 - INFO - Query processing completed successfully\n",
      "2025-03-03 14:56:54,949 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:54,955 - INFO - Query processing completed successfully\n",
      "2025-03-03 14:56:55,769 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:56:59,968 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"RAG_test_NVIDIA\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_hallucination_evaluator],\n",
    "    experiment_prefix=\"RAPTOR\",\n",
    "    metadata={\"variant\": \"NVIDIA context, gpt-4o-mini\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3261a",
   "metadata": {},
   "source": [
    "### Type 3: Document Relevance to Question\n",
    "\n",
    "#### Eval flow\n",
    "\n",
    "We simply use an LLM-as-judge with an easily customized grader prompt: \n",
    "\n",
    "https://smith.langchain.com/hub/langchain-ai/rag-document-relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39d7e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the LANGSMITH_API_KEY environment variable (create key in settings)\n",
    "from langchain import hub\n",
    "grade_prompt_doc_relevance = hub.pull(\"doc_question_relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe7068be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade prompt \n",
    "# grade_prompt_doc_relevance = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
    "\n",
    "def docs_relevance_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for document relevance\n",
    "    \"\"\"\n",
    "    \n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    contexts = run.outputs[\"contexts\"]\n",
    "        \n",
    "    # RAG answer \n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    # llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    \n",
    "    llm = model\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_doc_relevance | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\":input_question,\n",
    "                                  \"documents\":contexts})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"document_relevance\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33f9e165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAPTOR-19c10d49' at:\n",
      "https://smith.langchain.com/o/f09c54fc-44b8-4ffe-9055-bc40601f9137/datasets/7fad5e82-7b2d-438e-90e6-1e232f388896/compare?selectedSessions=a933762d-6704-4bb0-9303-a7274b7018a8\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b579fbef5a477e9c0f6f295de1b783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:59:16,480 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-03 14:59:16,482 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-03 14:59:16,484 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-03 14:59:16,486 - INFO - Processing query: In which large markets does NVIDIA apply its expertise and provide platforms?\n",
      "2025-03-03 14:59:16,487 - INFO - Processing query: What was NVIDIA's revenue in 2024 and 2023? Which year saw higher revenues and by how much?\n",
      "2025-03-03 14:59:16,488 - INFO - Processing query: Who is the CEO of NVIDIA?\n",
      "2025-03-03 14:59:16,877 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:16,892 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:16,967 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:17,596 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:18,217 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:18,228 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:18,340 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:18,682 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:18,930 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:19,095 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:19,179 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:19,478 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:19,940 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:20,252 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:20,374 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:20,856 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:20,916 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:20,925 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:21,289 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:21,303 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:21,452 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:22,060 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:22,315 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:22,907 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:23,113 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:23,125 - INFO - Query processing completed successfully\n",
      "2025-03-03 14:59:23,627 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:23,796 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:24,865 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:24,877 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:25,890 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:26,862 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:28,210 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:28,219 - INFO - Query processing completed successfully\n",
      "2025-03-03 14:59:29,319 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:30,060 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:30,065 - INFO - Query processing completed successfully\n",
      "2025-03-03 14:59:32,341 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-03 14:59:33,775 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"RAG_test_NVIDIA\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[docs_relevance_evaluator],\n",
    "    experiment_prefix=\"RAPTOR\",\n",
    "    metadata={\"variant\": \"NVIDIA context, gpt-4o-mini\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d042fed",
   "metadata": {},
   "source": [
    "## Complete Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba3fa926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'RAPTOR-98deaee5' at:\n",
      "https://smith.langchain.com/o/f09c54fc-44b8-4ffe-9055-bc40601f9137/datasets/176f75c1-604f-435b-b0c7-9eda1bdef775/compare?selectedSessions=ac8d5f21-57a6-4025-b704-9b3b6755d7f1\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208f6fba8e16430c9757de917482a927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 15:16:27,973 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:27,976 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:27,978 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:27,979 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:27,981 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:27,982 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:27,985 - INFO - Processing query: What was NVIDIA's revenue in 2024 and 2023? Which year saw higher revenues and by how much?\n",
      "2025-03-04 15:16:27,986 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:27,988 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:27,989 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:27,990 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:27,991 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:27,993 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:27,994 - INFO - Processing query: Between January 29, 2023, and January 28, 2024, NVIDIA’s total assets increased by approximately $24.5 billion. Based on the balance sheet, which two line items contributed the most to this increase, and what does this suggest about the company’s capital deployment focus?\n",
      "2025-03-04 15:16:27,995 - INFO - Processing query: In fiscal year 2024, NVIDIA generated a substantial increase in operating cash flow compared to fiscal year 2023. However, despite this cash inflow, the company's cash balance at year-end only increased modestly. Analyze the main drivers behind this modest cash increase and discuss what this reveals about NVIDIA’s capital allocation priorities and potential financial strategy.\n",
      "2025-03-04 15:16:27,996 - INFO - Processing query: How does the conflict in Israel effect NVIDIA's business? How many people are affected? What proportion is that relative to NVIDIA's complete workforce?\n",
      "2025-03-04 15:16:27,997 - INFO - Processing query: Who joined NVIDIA latest out of the executive officers and when?\n",
      "2025-03-04 15:16:27,999 - INFO - Processing query: What types of costs does NVIDIA have, and how have these costs changed over time as a percentage of revenue?\n",
      "2025-03-04 15:16:28,005 - INFO - Processing query: What is the lifestory of NVIDIA’s CFO, was he/she already at the firm when the Arm Acquisition was terminated?\n",
      "2025-03-04 15:16:28,006 - INFO - Processing query: What was the working capital of NVIDIA in 2024? What does it indicate?\n",
      "2025-03-04 15:16:28,008 - INFO - Processing query: How does NVIDIA perform across different regions in the global market?\n",
      "2025-03-04 15:16:28,009 - INFO - Processing query: Are there more men or women in NVIDIA's workforce? Are there more technical or non-technical people hired?\n",
      "2025-03-04 15:16:28,010 - INFO - Processing query: How does NVIDIA ensure that its autonomous vehicle platform can meet the safety and complexity requirements of modern AV and EV systems?\n",
      "2025-03-04 15:16:28,011 - INFO - Processing query: How does NVIDIA’s approach to AI training and inferencing across data center-scale solutions demonstrate full-stack innovation, and how does NVIDIA’s ecosystem contribute to reinforcing its leadership position in AI?\n",
      "2025-03-04 15:16:28,420 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:28,555 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:28,564 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:28,605 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:28,672 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:28,761 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:28,769 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:28,769 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:28,801 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:28,831 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:29,120 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:29,649 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:30,289 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:30,300 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:31,606 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:31,710 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:31,790 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:32,988 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:32,998 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:33,018 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:33,280 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:33,472 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 15:16:33,777 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:34,015 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:34,287 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:34,369 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:34,640 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:34,725 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:34,923 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:34,984 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:35,246 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:35,514 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:35,804 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:36,019 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:36,030 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:36,113 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:36,124 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:36,349 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:36,349 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:36,483 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:36,486 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:37,358 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:37,531 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:37,604 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:37,819 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:37,868 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:37,913 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:37,936 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:37,964 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:38,202 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:38,373 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:38,667 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:38,762 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:38,913 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:39,091 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:39,567 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:39,622 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:39,713 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:39,927 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:39,955 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:40,081 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:40,585 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:40,651 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:40,787 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:40,821 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:40,839 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:40,853 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:41,180 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:41,202 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:41,574 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:41,671 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:41,766 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:41,920 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:41,921 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:41,966 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:41,967 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:42,619 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:42,698 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:43,231 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:43,236 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:16:43,238 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:43,244 - INFO - Processing query: List all key competitors identified in NVIDIA’s 10-K filing. What strategies, products, or initiatives are these competitors pursuing to compete with NVIDIA?\n",
      "2025-03-04 15:16:43,307 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:43,429 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:43,533 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:43,565 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:43,771 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:43,990 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:44,125 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:44,752 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:44,762 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:44,764 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:44,890 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:45,535 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 15:16:45,546 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:16:45,548 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:45,552 - INFO - Processing query: What was the net value of P&E in 2024 and how is it calculated in this specific report?\n",
      "2025-03-04 15:16:45,583 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:45,864 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:45,894 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:16:45,895 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:45,897 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:45,903 - INFO - Processing query: How does NVIDIA’s change in cash and cash equivalents between fiscal year-end 2023 and 2024 compare to the change in total liabilities over the same period? And what does it indicate?\n",
      "2025-03-04 15:16:45,911 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:46,031 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:46,218 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:46,357 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:46,792 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:46,803 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:46,815 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:16:46,817 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:46,822 - INFO - Processing query: List all people mentioned in the report. And what are their titles within the NVIDIA?\n",
      "2025-03-04 15:16:47,175 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:47,308 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:47,567 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:47,776 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:47,964 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:48,276 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:48,355 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:48,637 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:49,198 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:49,332 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:49,433 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:49,675 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:49,682 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:16:49,683 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:49,687 - INFO - Processing query: Which products and countries were affected by the recent U.S. export controls, and how did NVIDIA respond?\n",
      "2025-03-04 15:16:50,079 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:50,080 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:50,094 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:16:50,097 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:50,102 - INFO - Processing query: When did the CEO join NVIDIA?\n",
      "2025-03-04 15:16:50,308 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:50,365 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:50,448 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:50,789 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:51,027 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:51,048 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:51,645 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:51,691 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:51,807 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:51,878 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:51,962 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:52,086 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:52,230 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:52,358 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:52,519 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:52,740 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:52,991 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:53,146 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:53,321 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:53,422 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:53,490 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:53,492 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:53,597 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:53,787 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:53,849 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:53,900 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:54,216 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:54,642 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 15:16:54,805 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:55,035 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:55,154 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:55,445 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:56,001 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:56,100 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:56,156 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:56,573 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:56,724 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:56,987 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:57,004 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:57,216 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:57,221 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:16:57,223 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:57,228 - INFO - Processing query: In which large markets does NVIDIA apply its expertise and provide platforms?\n",
      "2025-03-04 15:16:57,495 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:57,559 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:58,155 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:58,274 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:58,359 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:58,577 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:58,669 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:59,099 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:59,288 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:59,366 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:59,456 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:59,495 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:59,511 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:16:59,513 - ERROR - Failed to use model_dump to serialize <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_community.vectorstores.faiss.FAISS'>)\n",
      "2025-03-04 15:16:59,518 - INFO - Processing query: Who is the CEO of NVIDIA?\n",
      "2025-03-04 15:16:59,566 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:59,814 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:16:59,840 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:00,109 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:01,215 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:01,216 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:01,217 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:01,217 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:01,218 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:01,260 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:01,590 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:01,927 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:01,934 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:17:02,022 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:02,523 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:02,685 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:03,065 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:03,071 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:17:03,072 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:03,082 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:03,882 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:03,908 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:04,193 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:04,283 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:05,316 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:05,363 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:05,429 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:06,229 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:06,389 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:06,394 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:17:06,405 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:06,615 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:06,743 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:07,093 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:07,094 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:07,097 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:07,111 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:17:07,119 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:17:07,449 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:07,781 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:07,782 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:08,174 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:08,381 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 15:17:08,845 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:09,353 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:09,813 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:09,814 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:09,820 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:17:09,824 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:17:10,735 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:11,988 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:11,993 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:17:12,422 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:12,670 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:13,570 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:14,525 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:14,531 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:17:15,655 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:16,586 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:16,720 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:17,462 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:17,467 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:17:18,038 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:20,711 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:23,226 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:26,504 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:26,787 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:26,794 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:17:32,443 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:32,449 - INFO - Query processing completed successfully\n",
      "2025-03-04 15:17:37,564 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:47,911 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:17:57,508 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:18:06,946 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:18:16,617 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:18:26,529 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:18:37,569 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:18:47,912 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:18:57,640 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:19:07,060 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:19:17,210 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:19:28,468 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:19:37,895 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:19:48,020 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:19:58,729 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:20:07,555 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:20:18,187 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:20:30,210 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:20:38,364 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:20:52,672 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:21:00,682 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:21:09,576 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:21:18,716 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:21:30,321 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:21:37,590 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:21:50,186 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:21:58,172 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:22:08,208 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:22:18,345 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:22:28,625 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:22:45,052 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:22:49,680 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:23:00,738 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:23:07,087 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:23:17,430 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:23:27,551 - INFO - Retrying request to /chat/completions in 0.480770 seconds\n",
      "2025-03-04 15:23:31,254 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:23:37,193 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:23:47,946 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:23:59,842 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:24:10,209 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:24:24,503 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:24:26,960 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:24:42,946 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:24:48,779 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:24:58,852 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:25:09,082 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:25:18,295 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:25:29,300 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 15:25:41,366 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:25:47,908 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:26:02,980 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:26:12,176 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:26:19,649 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:26:29,582 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:26:40,232 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 15:26:56,412 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "dataset_name = \"RAG_Eval_QA\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator, answer_hallucination_evaluator, docs_relevance_evaluator],\n",
    "    experiment_prefix=\"RAPTOR\",\n",
    "    metadata={\"variant\": \"NVIDIA context, gpt-4o-mini\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae210b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
